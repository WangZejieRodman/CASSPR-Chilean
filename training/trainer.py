import os
from datetime import datetime
import numpy as np
import torch
from torch import nn
import pickle
import tqdm
import pathlib
import json
import pandas as pd
from sklearn.neighbors import KDTree

from torch.utils.tensorboard import SummaryWriter

from eval.evaluate import evaluate, print_eval_stats
from misc.utils import MinkLocParams, get_datetime
from models.loss import make_loss
from models.model_factory import model_factory

VERBOSE = False


def load_geo_coordinates(dataset_folder):
    """åŠ è½½æ‰€æœ‰sessionçš„åœ°ç†åæ ‡æ•°æ®"""
    base_path = "/home/wzj/pan2/Chilean_Underground_Mine_Dataset_Many_Times/"
    runs_folder = "chilean/"

    geo_coords = {}  # key: filename, value: (northing, easting)

    print("æ­£åœ¨åŠ è½½åœ°ç†åæ ‡æ•°æ®...")
    # éå†æ‰€æœ‰å¯èƒ½çš„session
    loaded_sessions = 0
    for session in range(100, 210):  # æ¶µç›–è®­ç»ƒå’Œæµ‹è¯•session
        session_str = str(session)
        csv_path = f"{base_path}{runs_folder}{session_str}/pointcloud_locations_20m_10overlap.csv"

        try:
            df = pd.read_csv(csv_path)
            session_count = 0
            for _, row in df.iterrows():
                # æ„å»ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æ•°æ®é›†ä¸­çš„æ ¼å¼ä¿æŒä¸€è‡´
                filename = f"chilean/{session_str}/pointcloud_20m_10overlap/{row['timestamp']}.bin"
                geo_coords[filename] = (row['northing'], row['easting'])
                session_count += 1

            if session_count > 0:
                loaded_sessions += 1
                if loaded_sessions <= 5:  # åªæ‰“å°å‰5ä¸ªsessionçš„ä¿¡æ¯
                    print(f"  Session {session}: åŠ è½½äº† {session_count} ä¸ªç‚¹äº‘åæ ‡")

        except Exception as e:
            # æŸäº›sessionå¯èƒ½ä¸å­˜åœ¨ï¼Œè·³è¿‡
            continue

    print(f"æ€»å…±ä» {loaded_sessions} ä¸ªsessionsåŠ è½½äº† {len(geo_coords)} ä¸ªç‚¹äº‘çš„åœ°ç†åæ ‡")
    return geo_coords


def verify_dataset_sampling(params):
    """éªŒè¯æ•°æ®é›†é‡‡æ ·çš„åœ°ç†è·ç¦»å…³ç³»"""
    from datasets.dataset_utils import make_datasets

    print(f"\nğŸ” === æ•°æ®é›†é‡‡æ ·éªŒè¯ ===")

    # åŠ è½½åœ°ç†åæ ‡
    geo_coords = load_geo_coordinates(params.dataset_folder)

    if len(geo_coords) == 0:
        print("âŒ æœªèƒ½åŠ è½½ä»»ä½•åœ°ç†åæ ‡æ•°æ®ï¼Œè·³è¿‡éªŒè¯")
        return

    # åŠ è½½æ•°æ®é›†
    datasets = make_datasets(params, debug=False)
    train_dataset = datasets['train']

    print(f"æ•°æ®é›†åŒ…å« {len(train_dataset.queries)} ä¸ªæŸ¥è¯¢")

    # éšæœºé€‰æ‹©ä¸€äº›æŸ¥è¯¢è¿›è¡ŒéªŒè¯
    import random
    sample_queries = random.sample(list(train_dataset.queries.keys()), min(10, len(train_dataset.queries)))

    problem_count = 0
    total_checked = 0

    for query_idx in sample_queries[:5]:  # åªéªŒè¯å‰5ä¸ªï¼Œé¿å…è¾“å‡ºå¤ªå¤š
        query_file = train_dataset.queries[query_idx]['query']
        if query_file not in geo_coords:
            print(f"âš ï¸  æŸ¥è¯¢ {query_idx}: æ‰¾ä¸åˆ°æ–‡ä»¶ {query_file} çš„åœ°ç†åæ ‡")
            continue

        query_coord = geo_coords[query_file]
        positives = train_dataset.get_positives_ndx(query_idx)
        negatives = train_dataset.get_negatives_ndx(query_idx)

        print(f"\nğŸ“ æŸ¥è¯¢ {query_idx}: {query_file.split('/')[-1]}")
        print(f"   åæ ‡: ({query_coord[0]:.1f}, {query_coord[1]:.1f})")
        print(f"   æ­£æ ·æœ¬æ•°é‡: {len(positives)}, è´Ÿæ ·æœ¬æ•°é‡: {len(negatives)}")

        # æ£€æŸ¥å‰å‡ ä¸ªæ­£æ ·æœ¬çš„åœ°ç†è·ç¦»
        pos_problems = 0
        for i, pos_idx in enumerate(list(positives)[:3]):
            pos_file = train_dataset.queries[pos_idx]['query']
            if pos_file in geo_coords:
                pos_coord = geo_coords[pos_file]
                dist = np.sqrt((query_coord[0] - pos_coord[0]) ** 2 + (query_coord[1] - pos_coord[1]) ** 2)
                status = "âœ…" if dist < 7 else "âŒ"
                if dist >= 7:
                    pos_problems += 1
                    problem_count += 1
                print(f"     æ­£æ ·æœ¬ {i}: è·ç¦»={dist:.1f}ç±³ {status}")
                total_checked += 1

        # æ£€æŸ¥å‰å‡ ä¸ªè´Ÿæ ·æœ¬çš„åœ°ç†è·ç¦»
        neg_problems = 0
        for i, neg_idx in enumerate(list(negatives)[:3]):
            neg_file = train_dataset.queries[neg_idx]['query']
            if neg_file in geo_coords:
                neg_coord = geo_coords[neg_file]
                dist = np.sqrt((query_coord[0] - neg_coord[0]) ** 2 + (query_coord[1] - neg_coord[1]) ** 2)
                status = "âœ…" if dist > 35 else "âŒ"
                if dist <= 35:
                    neg_problems += 1
                    problem_count += 1
                print(f"     è´Ÿæ ·æœ¬ {i}: è·ç¦»={dist:.1f}ç±³ {status}")
                total_checked += 1

        if pos_problems > 0 or neg_problems > 0:
            print(f"   âš ï¸  æ­¤æŸ¥è¯¢å­˜åœ¨é—®é¢˜: {pos_problems} ä¸ªé”™è¯¯æ­£æ ·æœ¬, {neg_problems} ä¸ªé”™è¯¯è´Ÿæ ·æœ¬")

    # æ€»ç»“
    print(f"\nğŸ“Š === éªŒè¯æ€»ç»“ ===")
    print(f"æ£€æŸ¥äº† {total_checked} ä¸ªæ ·æœ¬å…³ç³»")
    print(f"å‘ç° {problem_count} ä¸ªé—®é¢˜æ ·æœ¬ ({problem_count / total_checked * 100:.1f}%)")

    if problem_count == 0:
        print("âœ… æ•°æ®é›†çš„æ­£è´Ÿæ ·æœ¬åœ°ç†å…³ç³»æ­£ç¡®")
    elif problem_count < total_checked * 0.1:
        print("âš ï¸  å‘ç°å°‘é‡é—®é¢˜ï¼Œå¯èƒ½æ˜¯æ•°æ®è¾¹ç•Œæƒ…å†µ")
    else:
        print("âŒ å‘ç°å¤§é‡é—®é¢˜ï¼Œæ•°æ®é›†å¯èƒ½å­˜åœ¨ç³»ç»Ÿæ€§é”™è¯¯")


def print_stats(stats, phase):
    if 'num_pairs' in stats:
        # For batch hard contrastive loss
        s = '{} - Mean loss: {:.6f}    Avg. embedding norm: {:.4f}   Pairs per batch (all/non-zero pos/non-zero neg): {:.1f}/{:.1f}/{:.1f}'
        print(s.format(phase, stats['loss'], stats['avg_embedding_norm'], stats['num_pairs'],
                       stats['pos_pairs_above_threshold'], stats['neg_pairs_above_threshold']))
    elif 'num_triplets' in stats:
        # For triplet loss
        s = '{} - Mean loss: {:.6f}    Avg. embedding norm: {:.4f}   Triplets per batch (all/non-zero): {:.1f}/{:.1f}'
        print(s.format(phase, stats['loss'], stats['avg_embedding_norm'], stats['num_triplets'],
                       stats['num_non_zero_triplets']))
    elif 'num_pos' in stats:
        s = '{} - Mean loss: {:.6f}    Avg. embedding norm: {:.4f}   #positives/negatives: {:.1f}/{:.1f}'
        print(s.format(phase, stats['loss'], stats['avg_embedding_norm'], stats['num_pos'], stats['num_neg']))

    s = ''
    l = []
    if 'mean_pos_pair_dist' in stats:
        s += 'Pos dist (min/mean/max): {:.4f}/{:.4f}/{:.4f}   Neg dist (min/mean/max): {:.4f}/{:.4f}/{:.4f}'
        l += [stats['min_pos_pair_dist'], stats['mean_pos_pair_dist'], stats['max_pos_pair_dist'],
              stats['min_neg_pair_dist'], stats['mean_neg_pair_dist'], stats['max_neg_pair_dist']]
    if 'pos_loss' in stats:
        if len(s) > 0:
            s += '   '
        s += 'Pos loss: {:.4f}  Neg loss: {:.4f}'
        l += [stats['pos_loss'], stats['neg_loss']]
    if len(l) > 0:
        print(s.format(*l))


def tensors_to_numbers(stats):
    stats = {e: stats[e].item() if torch.is_tensor(stats[e]) else stats[e] for e in stats}
    return stats


def do_train(dataloaders, params: MinkLocParams, ckpt=None, debug=False, visualize=False):
    # Create model class
    s = get_datetime()
    now = datetime.now()
    now_strftime = now.strftime("%Y%m%d-%H%M%S")
    model = model_factory(params)
    start_epoch = 1
    if ckpt is not None:
        checkpoint = torch.load(ckpt)
        model.load_state_dict(checkpoint)
        print("Loaded model from ", ckpt)
        start_epoch = int(ckpt.split('/')[-1].split('.')[0].split('epoch')[-1]) + 1
        print("Starting from epoch ", start_epoch)

    model_name = 'model_' + params.model_params.backbone + params.model_params.pooling + \
                 '_' + now_strftime.replace('-', '_')
    print('Model name: {}'.format(model_name))
    weights_path = create_weights_folder()
    model_pathname = os.path.join(weights_path, model_name)
    pathlib.Path(model_pathname).mkdir(parents=False, exist_ok=True)

    # åˆå§‹åŒ–Lossç»Ÿè®¡æ–‡ä»¶
    loss_stats_file = "æ¯è½®Lossç»Ÿè®¡.txt"
    with open(loss_stats_file, 'w', encoding='utf-8') as f:
        f.write("Epoch\tAverage_Loss\tMin_Loss\tMax_Loss\tTotal_Batches\n")

    if hasattr(model, 'print_info'):
        model.print_info()
    else:
        n_params = sum([param.nelement() for param in model.parameters()])
        print('Number of model parameters: {}'.format(n_params))

    # Move the model to the proper device before configuring the optimizer
    if torch.cuda.is_available():
        device = torch.device(f"cuda:{params.model_params.gpu}")
        torch.cuda.set_device(device)
        model.to(device)
    else:
        device = "cpu"

    print('Model device: {}'.format(device))

    loss_fn = make_loss(params)

    # Training elements
    if params.weight_decay is None or params.weight_decay == 0:
        optimizer = torch.optim.Adam(model.parameters(), lr=params.lr)
    else:
        optimizer = torch.optim.Adam(model.parameters(), lr=params.lr, weight_decay=params.weight_decay)

    if params.scheduler is None:
        scheduler = None
    else:
        if params.scheduler == 'CosineAnnealingLR':
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params.epochs + 1,
                                                                   eta_min=params.min_lr)
        elif params.scheduler == 'MultiStepLR':
            scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, params.scheduler_milestones, gamma=0.5)
        else:
            raise NotImplementedError('Unsupported LR scheduler: {}'.format(params.scheduler))

    ###########################################################################
    # Initialize TensorBoard writer
    ###########################################################################

    logdir = os.path.join("../tf_logs", now_strftime)
    writer = SummaryWriter(logdir)

    ###########################################################################
    #
    ###########################################################################

    is_validation_set = 'val' in dataloaders
    if is_validation_set:
        phases = ['train', 'val']
    else:
        phases = ['train']

    # Training statistics
    stats = {'train': [], 'val': [], 'eval': []}

    for epoch in tqdm.tqdm(range(start_epoch, params.epochs + 1)):
        for phase in phases:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_stats = []  # running stats for the current epoch
            batch_losses = []  # æ–°å¢ï¼šè®°å½•æ¯ä¸ªbatchçš„loss

            count_batches = 0

            # æ·»åŠ æ•°æ®è´¨é‡æ£€æŸ¥ - åªåœ¨ç¬¬ä¸€ä¸ªepochæ£€æŸ¥
            if epoch == 1 and phase == 'train':
                print("\n=== æ•°æ®è´¨é‡æ£€æŸ¥ ===")

            for batch, positives_mask, negatives_mask in dataloaders[phase]:
                # batch is (batch_size, n_points, 3) tensor
                # labels is list with indexes of elements forming a batch
                count_batches += 1
                batch_stats = {}

                if debug and count_batches > 2:
                    break

                # Move everything to the device except 'coords' which must stay on CPU
                batch = {e: batch[e].to(device) if e != 'coords' else batch[e] for e in batch}

                n_positives = torch.sum(positives_mask).item()
                n_negatives = torch.sum(negatives_mask).item()

                # æ•°æ®è´¨é‡è°ƒè¯•ä¿¡æ¯ - åªåœ¨ç¬¬ä¸€ä¸ªepochçš„å‰10ä¸ªbatchæ£€æŸ¥
                if epoch == 1 and phase == 'train' and count_batches <= 10:
                    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æ­£è´Ÿæ ·æœ¬æ•°é‡
                    pos_per_sample = torch.sum(positives_mask, dim=1)
                    neg_per_sample = torch.sum(negatives_mask, dim=1)

                    print(f"Batch {count_batches}: positives={n_positives}, negatives={n_negatives}")
                    print(f"ğŸ” è¯¦ç»†åˆ†æ Batch {count_batches}:")
                    print(f"   Total positives: {n_positives}, Total negatives: {n_negatives}")
                    print(
                        f"   Pos per sample: min={pos_per_sample.min().item()}, max={pos_per_sample.max().item()}, mean={pos_per_sample.float().mean().item():.1f}")
                    print(
                        f"   Neg per sample: min={neg_per_sample.min().item()}, max={neg_per_sample.max().item()}, mean={neg_per_sample.float().mean().item():.1f}")

                if n_positives == 0 or n_negatives == 0:
                    # Skip a batch without positives or negatives
                    print('WARNING: Skipping batch without positive or negative examples')
                    continue
                else:
                    if epoch == 1 and phase == 'train' and count_batches <= 10:
                        print(f"Processing batch {count_batches}: {n_positives} positives, {n_negatives} negatives")

                optimizer.zero_grad()
                if visualize:
                    # visualize_batch(batch)
                    pass

                with torch.set_grad_enabled(phase == 'train'):
                    # Compute embeddings of all elements
                    torch.autograd.set_detect_anomaly(True)
                    embeddings = model(batch)
                    loss, temp_stats, _ = loss_fn(embeddings, positives_mask, negatives_mask)

                    temp_stats = tensors_to_numbers(temp_stats)
                    batch_stats.update(temp_stats)
                    batch_stats['loss'] = loss.item()

                    # æ–°å¢ï¼šè®°å½•batch loss
                    if phase == 'train':
                        batch_losses.append(loss.item())

                    if phase == 'train':
                        # æ·»åŠ æ¢¯åº¦è£å‰ª
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        loss.backward()
                        optimizer.step()

                running_stats.append(batch_stats)
                torch.cuda.empty_cache()  # Prevent excessive GPU memory consumption by SparseTensors

            # ******* PHASE END *******
            # è®¡ç®—epochç»Ÿè®¡å¹¶å†™å…¥æ–‡ä»¶
            if phase == 'train' and batch_losses:
                avg_loss = np.mean(batch_losses)
                min_loss = np.min(batch_losses)
                max_loss = np.max(batch_losses)
                total_batches = len(batch_losses)

                # å†™å…¥Lossç»Ÿè®¡æ–‡ä»¶
                with open(loss_stats_file, 'a', encoding='utf-8') as f:
                    f.write(f"{epoch}\t{avg_loss:.6f}\t{min_loss:.6f}\t{max_loss:.6f}\t{total_batches}\n")

                print(
                    f"Epoch {epoch} Loss Statistics - Avg: {avg_loss:.6f}, Min: {min_loss:.6f}, Max: {max_loss:.6f}, Batches: {total_batches}")

            # Compute mean stats for the epoch
            epoch_stats = {}
            for key in running_stats[0].keys():
                temp = [e[key] for e in running_stats]
                epoch_stats[key] = np.mean(temp)

            stats[phase].append(epoch_stats)
            print_stats(epoch_stats, phase)

        # ******* EPOCH END *******

        if scheduler is not None:
            scheduler.step()

        loss_metrics = {'train': stats['train'][-1]['loss']}
        if 'val' in phases:
            loss_metrics['val'] = stats['val'][-1]['loss']
        writer.add_scalars('Loss', loss_metrics, epoch)

        if 'num_triplets' in stats['train'][-1]:
            nz_metrics = {'train': stats['train'][-1]['num_non_zero_triplets']}
            if 'val' in phases:
                nz_metrics['val'] = stats['val'][-1]['num_non_zero_triplets']
            writer.add_scalars('Non-zero triplets', nz_metrics, epoch)

        elif 'num_pairs' in stats['train'][-1]:
            nz_metrics = {'train_pos': stats['train'][-1]['pos_pairs_above_threshold'],
                          'train_neg': stats['train'][-1]['neg_pairs_above_threshold']}
            if 'val' in phases:
                nz_metrics['val_pos'] = stats['val'][-1]['pos_pairs_above_threshold']
                nz_metrics['val_neg'] = stats['val'][-1]['neg_pairs_above_threshold']
            writer.add_scalars('Non-zero pairs', nz_metrics, epoch)

        if params.batch_expansion_th is not None:
            # Dynamic batch expansion
            epoch_train_stats = stats['train'][-1]
            if 'num_non_zero_triplets' not in epoch_train_stats:
                print('WARNING: Batch size expansion is enabled, but the loss function is not supported')
            else:
                # Ratio of non-zero triplets
                rnz = epoch_train_stats['num_non_zero_triplets'] / epoch_train_stats['num_triplets']
                if rnz < params.batch_expansion_th:
                    dataloaders['train'].batch_sampler.expand_batch()

        print('')

        if params.dataset_name != 'TUM' and epoch % 10 != 0 and epoch > 0 and epoch < params.epochs:
            continue

        # Save model weights
        final_model_path = os.path.join(model_pathname, f'epoch{epoch}.pth')
        torch.save(model.state_dict(), final_model_path)

        # Evaluate the final model
        model.eval()
        with torch.no_grad():
            final_eval_stats = evaluate(model, device, params)
        print(f'\nEpoch{epoch} model:')
        print_eval_stats(final_eval_stats)
        stats['eval'].append({f'epoch{epoch}': final_eval_stats})
        print('')
        for database_name in final_eval_stats:
            nz_metric1 = {f'{database_name}': final_eval_stats[database_name]['ave_one_percent_recall'].item()}
            nz_metric2 = {f'{database_name}': final_eval_stats[database_name]['ave_recall'][0].item()}
            writer.add_scalars('Eval Mean recall', nz_metric2, epoch)
            writer.add_scalars('Eval One percent recall', nz_metric1, epoch)
            writer.flush()

    stats = {'stats': stats, 'params': vars(params)}
    with open(os.path.join(logdir, 'config.json'), 'w') as f:
        json.dump(stats, f, indent=6, default=lambda o: getattr(o, '__dict__', str(o)))


def export_eval_stats(file_name, prefix, eval_stats, dataset_name):
    s = '\n' + prefix + '\n\n'
    ave_1p_recall_l = []
    ave_recall_l = []
    # Print results on the final model
    with open(file_name, "a") as f:
        if dataset_name == 'USyd':
            ave_1p_recall = eval_stats['usyd']['ave_one_percent_recall']
            ave_1p_recall_l.append(ave_1p_recall)
            ave_recall = eval_stats['usyd']['ave_recall'][0]
            ave_recall_l.append(ave_recall)
            s += ", {:0.2f}, {:0.2f}".format(ave_1p_recall, ave_recall)

        elif dataset_name == 'IntensityOxford':
            ave_1p_recall = eval_stats['intensityOxford']['ave_one_percent_recall']
            ave_1p_recall_l.append(ave_1p_recall)
            ave_recall = eval_stats['intensityOxford']['ave_recall'][0]
            ave_recall_l.append(ave_recall)
            s += ", {:0.2f}, {:0.2f}".format(ave_1p_recall, ave_recall)
        elif dataset_name == 'TUM':
            ave_1p_recall = eval_stats['tum']['ave_one_percent_recall']
            ave_1p_recall_l.append(ave_1p_recall)
            ave_recall = eval_stats['tum']['ave_recall'][0]
            ave_recall_l.append(ave_recall)
            s += 'Average 1% recall: {:0.2f}\n'.format(ave_1p_recall)
            s += 'Average Recall: {:0.2f}\n'.format(ave_recall)
        else:
            for ds in ['oxford', 'university', 'residential', 'business']:
                ave_1p_recall = eval_stats[ds]['ave_one_percent_recall']
                ave_1p_recall_l.append(ave_1p_recall)
                ave_recall = eval_stats[ds]['ave_recall'][0]
                ave_recall_l.append(ave_recall)
                s += ", {:0.2f}, {:0.2f}".format(ave_1p_recall, ave_recall)

        mean_1p_recall = np.mean(ave_1p_recall_l)
        mean_recall = np.mean(ave_recall_l)
        s += 'Mean 1% Recall @N: {:0.2f}\nMean Recall {:0.2f}\n\n '.format(mean_1p_recall, mean_recall)
        f.write(s)


def create_weights_folder():
    # Create a folder to save weights of trained models
    this_file_path = pathlib.Path(__file__).parent.absolute()
    temp, _ = os.path.split(this_file_path)
    weights_path = os.path.join(temp, 'weights')
    if not os.path.exists(weights_path):
        os.mkdir(weights_path)
    assert os.path.exists(weights_path), 'Cannot create weights folder: {}'.format(weights_path)
    return weights_path
